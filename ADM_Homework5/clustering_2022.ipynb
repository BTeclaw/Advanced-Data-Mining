{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the exercise, experiments will be carried out on both artificial and real data. The following functions are suitable for generating artificial data:\n",
    "\n",
    "* `make_classification` - generates a given number of points (`n_samples`) belonging to a predefined number of classes (`n_classes`) in a feature space with a given number of dimensions (`n_features`) \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html\n",
    "\n",
    "* `make_blobs` - generates a given number of normally distributed clusters (`centers`) containing a given number of points (`n_samples`) in a space with a given number of dimensions (`n_features`) \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import make_blobs, make_classification\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x, y = make_blobs(n_features=2, centers=5)\n",
    "x, y =  make_classification(n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, n_classes=3)\n",
    "plt.scatter(x[:,0],x[:,1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `KMeans` function groups the data into a given number of clusters (`n_clusters`). The algorithm is run a given number of times (parameter `n_init`, default 10) for different initial locations of cluster centers. Finally, the best solution is selected, i.e. the one for which the average distance of points from cluster centers is the smallest. The `get_params` attribute contains the method parameters. \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeans(n_clusters=3)\n",
    "print(k_means.get_params)\n",
    "t0 = time.time()\n",
    "k_means.fit(x)\n",
    "print(\"Training time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final position of cluster centers may be read form `cluster_centers_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clustering, any data point can be assigned to the closest cluster using the `predict` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = k_means.predict(x)\n",
    "plt.scatter(x[:,0],x[:,1],c = y_predict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 0**\n",
    "\n",
    "Generate a few sample data sets with a different structure, in which the following situations will arise:\n",
    "* a set containing several clusters with normal distributions;\n",
    "* a set containing outlier examples;\n",
    "* a set in which one of the classes is \"stretched\" (elongated).\n",
    "\n",
    "To use the generated sets in further experiments, save them in files, e.g. in the following way: \n",
    "\n",
    "`pd.DataFrame(dane).to_csv(\"file1.csv\", index=False)`\n",
    "\n",
    "You may read the saved data using the following statement:\n",
    "\n",
    "`dataset = pd.read_csv(\"file1.csv\").values`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 (1,5 pt)**\n",
    "\n",
    "Perform k-means clustering for the generated data sets, determining in advance the number of clusters for each data set. Without changing the number of clusters, experiment with different values of the input parameters, which you can read about in the documentation. Investigate which of the input parameters of the method can positively affect the efficiency of the algorithm, both in terms of clustering quality and speed. As a measure of clustering quality, take the `inertia_` attribute, which is the sum of the squared distances of sample points from the cluster centers (the smaller the value, the better). As a measure of speed take the `n_iter` attribute which is the number of iterations performed. The results of the experiments should be visible, i.e. do not print the final conclusion only, but also the results leading to the conclusions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating the quality of clustering**\n",
    "\n",
    "There are some cluster validity indices.\n",
    "1. If some labels are given (e.g. class assignements), you may apply Adjusted Round Index (ARI) (`sklearn.metrics.adjusted_rand_score`) taking values from [-1, 1].\n",
    "2. if class assigment is not known (which is usually the case while performing clustering), you may apply silhouette coefficient (`sklearn.metrics.silhouette_score`) taking values from [-1, 1].\n",
    "\n",
    "The description of all measures may be found at\n",
    "http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(\"The value of ARI: \",metrics.adjusted_rand_score(y, y_predict)) \n",
    "print(\"The value of SILHOUETTE coefficient: \",metrics.silhouette_score(x, y_predict, metric='euclidean')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excersise 2 (0,5 pt)**\n",
    "\n",
    "For the generated data sets, plot a graph showing the relationship between the values of silhouette coefficient and the number of detected clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first experiments will be performed on the `digits` dataset. In order to be able to demonstrate the clustering results on a 2D plot, the `digits` data will first be transformed into two-dimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "digits = load_digits(n_class=10)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = X.shape\n",
    "X_2D = PCA(n_components=2).fit_transform(X) #transforming multidimensional data to 2D\n",
    "plt.title(\"Data before clustering. Colors refer to classes (digits).\")\n",
    "plt.scatter(X_2D[:,0], X_2D[:,1],c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering may be performed using `AgglomerativeClustering` function.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "\n",
    "\n",
    "Parameter `linkage` defines the way a distance between clusters is calculated. It can be as follows:\n",
    "* `ward` - minimizing variance of the clusters;\n",
    "* `average` - average distance between pairs of points from two clusters;\n",
    "* `complete` - maximum distance between pairs of points from two clusters;\n",
    "* `single` - minimum distance between pairs of points from two clusters.\n",
    "\n",
    "See below how the method works for `digits` data for the above four cases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = AgglomerativeClustering(linkage='ward', n_clusters=10)\n",
    "h.fit(X_2D)\n",
    "plt.title(\"WARD (colors refer to identified clusters)\")\n",
    "plt.scatter(X_2D[:,0], X_2D[:,1],c=h.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = AgglomerativeClustering(linkage='average', n_clusters=10)\n",
    "h.fit(X_2D)\n",
    "plt.title(\"AVERAGE (colors refer to identified clusters)\")\n",
    "plt.scatter(X_2D[:,0], X_2D[:,1],c=h.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = AgglomerativeClustering(linkage='complete', n_clusters=10)\n",
    "h.fit(X_2D)\n",
    "plt.title(\"COMPLETE (colors refer to identified clusters)\")\n",
    "plt.scatter(X_2D[:,0], X_2D[:,1],c=h.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = AgglomerativeClustering(linkage='single', n_clusters=10)\n",
    "h.fit(X_2D)\n",
    "plt.title(\"SINGLE (colors refer to identified clusters)\")\n",
    "plt.scatter(X_2D[:,0], X_2D[:,1],c=h.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 3 (0,5 pt)**\n",
    "\n",
    "For the above four clustering results obtained for digits data, calculate the value of Adjusted Round Index (ARI) (`sklearn.metrics.adjusted_rand_score`) to evaluate the consistency of the obtained clustering and real labels. Print and comment  the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 4 (1 pt)**\n",
    "\n",
    "Perform hierarchical clustering for your data sets. Compare the performance of the four versions of hierarchical clustering. Can you explain in what situations each variant performs better than other ones (write down your observations)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 5 (0,5 pt)**\n",
    "\n",
    "For one of the hierarchical methods (`single`, `complete` or `average`) and one of data sets (either you own data set or `digits`) compare the results of the method using various distance measures: `euclidean`, `manhattan`, `cosine` (`affinity` parameter). Write down your observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis of the obtained hierarchical structure of clustering**\n",
    "\n",
    "It is possible to analyze the structure of clustering from attribute `children_`. The structure description will be analysed below for an example data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_blobs(n_samples = 20, n_features=2, centers=2)\n",
    "h = AgglomerativeClustering(linkage='average', n_clusters=2)\n",
    "h.fit(x)\n",
    "plt.title(\"AVERAGE\")\n",
    "plt.scatter(x[:,0], x[:,1],c=h.labels_)\n",
    "plt.show()\n",
    "h.children_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows in `children_` array is one less than the number of elements in the data set. Subsequent rows correspond to iterations of the algorithm and they contain indices of joined elements. In our example there are 20 samples in the data set. If a value lower than 20 appears in the array, it means a sample of that index is linked in the given iteration. Values greater than 20 are indices of clusters that contain more than one sample. The index of a cluster created in a given iteration is equal to the index of the row increased by the number of elements in the set. For example, if in the third row of the aray we had [5,11], it would mean that in the third iteration samples 5 and 11 were combined into one cluster, which was assigned index 22 (2 + 20)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering has been also implemented in `scipy` package. In this case a dendrogram may be also drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "h2 = hierarchy.average(x)\n",
    "den = hierarchy.dendrogram(h2, no_labels=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Excercise 6 (2 pt)**\n",
    "\n",
    "On the basis of hierarchical clustering, propose and implement your own method of detecting outliers. Outliers are data samples that do not match the rest of the set. Briefly describe the idea of your method. Test it on example sets of two-dimensional data created by you in Excercise 0 and present the results on a scatter plot (e.g. highlight the detected unusual examples with a different color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=2, min_samples=3)\n",
    "dbscan.fit(X_2D) #X_2D - digits data previously transformed to 2D\n",
    "\n",
    "plt.title(\"DBSCAN\")\n",
    "plt.scatter(X_2D[:,0], X_2D[:,1],c = dbscan.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering results (labels) are saved in `dbscan.labels_`. Label value -1 indicates that the corresponding point has not been assigned to any cluster, which means it's an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 7 (1,5 pt)**\n",
    "\n",
    "For each of your own sets of data perform DBSCAN clustering. In each case adjust the algorithm parameters (`eps`, `min_samples`) to obtain the desired number of clusters, i.e. the number you set while using `make_calssification` or `make_blobs` functions. Show the results on scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be analyzing a collection of documents containing news from 20 newsgroups. To speed up the operation of the methods, we will limit to four selected categories (groups). Choose four categories by uncommenting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = [\n",
    "    #'alt.atheism',\n",
    "    'comp.graphics',\n",
    "    #'comp.os.ms-windows.misc',\n",
    "    #'comp.sys.ibm.pc.hardware',\n",
    "    #'comp.sys.mac.hardware',\n",
    "    'comp.windows.x',\n",
    "    #'misc.forsale',\n",
    "    #'rec.autos',\n",
    "    #'rec.motorcycles',\n",
    "    #'rec.sport.baseball',\n",
    "    #'rec.sport.hockey',\n",
    "    #'sci.crypt',\n",
    "    'sci.electronics',\n",
    "    #'sci.med',\n",
    "    'sci.space',\n",
    "    #'soc.religion.christian',\n",
    "    #'talk.politics.guns',\n",
    "    #'talk.politics.mideast',\n",
    "    #'talk.politics.misc',\n",
    "    #'talk.religion.misc'\n",
    "]\n",
    "dataset = fetch_20newsgroups(subset='all', categories = categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of loaded documents is \", len(dataset.data))\n",
    "print(\"They belong to one of \", len(dataset.target_names), \" categories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example news from the set is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_number = 5\n",
    "print(\"Example news from \", dataset.target_names[dataset.target[doc_number]])\n",
    "dataset.data[doc_number]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document will be represented by a feature vector whose components are weights for the words considered in the analyzed set of documents. Class `TfidfVectorizer` is used to calcualate these feature vectors.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer\n",
    "\n",
    "The number of features describing documents may, in an extreme case, be equal to the number of all words appearing in all documents in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features = None, stop_words='english')\n",
    "X = vectorizer.fit_transform(dataset.data)\n",
    "print(\"Number of documents: %d\\nNumber of features: %d\" %X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given document, the i-th component of a feature vector is the number of occurrences of the i-th word (feature) in this document multiplied by the idf coefficient, which depends on the total number of documents in the set and the number of documents containing the i-th word. A detailed description of determining these features can be found in the documentation\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html.\n",
    "The vector of idf values for all words (features) can be read as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_feature_names` method allows you to read the names of all features, i.e. the words taken into account when analyzing a set of documents. They are sorted alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "terms[-20:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 8 (1.5 pt)**\n",
    "\n",
    "\n",
    "Choose one of the clustering methods you know, adjust its parameters and cluster documents from the given collection. Calculate the value of a selected cluster validity index.\n",
    "1. for different number of clusters\n",
    "2. for different number of features describing the documents (parameter `max_features`)\n",
    "\n",
    "Adjust the values of the above two parameters (number of clusters, number of features) to obtain the best possible clustering (from the point of view of the selected cluster validity index). You may present the results of the analysis on plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 9 (0,5 pt)**\n",
    "\n",
    "For each detected cluster, display 10 most significant features (words), i.e. 10 words with the highest values for the corresponding features. When analyzing the feature values, take into account only the cluster centers which are good representatives of each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 10 (0.5 pt)**\n",
    "\n",
    "Enter a sample short text (2-3 sentences) on any topic. The text should be an element of an array. Convert the text to a feature vector (`vectorizer.transfom`). Verify which cluster this text is assigned to. Experiment with at  least three texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
