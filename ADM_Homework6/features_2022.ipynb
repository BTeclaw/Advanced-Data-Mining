{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following excercises `fetch20newsgroups` wil bee analyzed. Features representing the documents will be evaluated from the point of view of a classifiaction task using different feature evaluation criteria. \n",
    "\n",
    "Choose (uncomment) several (3-5) groups from the data set. These groups will be regarded as different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'comp.graphics',\n",
    "    'comp.os.ms-windows.misc',\n",
    "    'comp.sys.ibm.pc.hardware',\n",
    "    'comp.sys.mac.hardware',\n",
    "    'comp.windows.x',\n",
    "    'misc.forsale',\n",
    "    #'rec.autos',\n",
    "    #'rec.motorcycles',\n",
    "    #'rec.sport.baseball',\n",
    "    #'rec.sport.hockey',\n",
    "    #'sci.crypt',\n",
    "    #'sci.electronics',\n",
    "    #'sci.med',\n",
    "    'sci.space',\n",
    "    #'soc.religion.christian',\n",
    "    #'talk.politics.guns',\n",
    "    #'talk.politics.mideast',\n",
    "    #'talk.politics.misc',\n",
    "    #'talk.religion.misc'\n",
    "]\n",
    "dataset = fetch_20newsgroups(subset='all', categories = categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting documents to feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, max_features = None, stop_words='english')\n",
    "docs_vectors = vectorizer.fit_transform(dataset.data)\n",
    "print(\"Number of documents: %d\\nNumber of features: %d\" %docs_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into trainig and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, target_train, target_test = train_test_split(docs_vectors, dataset.target, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "tree = tree.DecisionTreeClassifier()\n",
    "tree.fit(data_train, target_train)\n",
    "print(\"Classification error for the test set is: \", 1.0 - tree.score(data_test,target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform feature selection class `SelectKBest` may be used. It lets evaluate features on the basis of a mearue given as a function argument. \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\n",
    "\n",
    "The `fit` method calculates the measure value for each feature. These values may be read from attribute `scores_`.\n",
    "\n",
    "The `transform` method modifies data set by leaving only given number of features, which achieved the highest scores.\n",
    "\n",
    "The `fit_transform` method performs both scoring features and modifying data set. The same effect may be obtained after applying `fit` followed by `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2\n",
    "select_mutual = SelectKBest(mutual_info_classif, k = 100)\n",
    "select_mutual.fit(data_train, target_train)\n",
    "select_mutual.scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 1 (2,5 pt)**\n",
    "\n",
    "Modify the data set by leaving only N features, which achieve the highest values of chi2 score.  Train and test the decision tree after the modifiation. Does feature reduction change the value of classification error? Find the optimal value of N, i.e. the value minimizing classification error. Remember, that test data shoud never take part in adjusting the number of features. It means you need to split data into training and testing subsets first. Adjust N on the basis of training data in a cross-validation procedure. Then train the final tree for the identified optimal N on the basis of training subset and test it on the basis of test subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 2 (1 pt)**\n",
    "\n",
    "Display 30 words regarding features achieving the highest scores of mutual information. Can you see the relation between the words and the classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a method commonly used to extract uncorrelated features. It is omplemented in `PCA` class. The `n_components` parameter of the method defines the number of extracted new features.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "digits = load_digits(n_class=10)\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "n_samples, n_features = X.shape\n",
    "pca = PCA(n_components = 2)\n",
    "X_new = pca.fit_transform(X)\n",
    "plt.scatter(X_new[:,0], X_new[:,1],c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear discriminant analysis (LDA) is another feature extraction method. In contrast to PCA, it takes into account information about classes.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "plt.scatter(X_lda[:,0], X_lda[:,1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 3 (1 pt)**\n",
    "\n",
    "For `digits` data set, perform PCA and plot a graph showing the variance values for successive principal components. The graph should be drawn for all components. The variance values can be read from the `pca.explained_variance_ratio_` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 4 (1,5 pt)**\n",
    "\n",
    "Write a function that returns the number of features that need to be included in order to retain a given amount of variance (sum of variances associated with each component) after performing PCA. The amount of variance should be given as a parameter of the function. This parameter may take values from the range (0; 1]. If the input parameter is 1, which means 100% variance, the function should return the maximum possible number of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fetch_olivetti_faces` data set contains 400 images of size 64x64 presenting 40 people. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "dataset = fetch_olivetti_faces(shuffle=True)\n",
    "faces = dataset.data\n",
    "n_samples, n_features = faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (64, 64)\n",
    "plt.figure(figsize = (3 * 4, 3* 100))\n",
    "for i, image in enumerate(faces[:20,:]):\n",
    "    plt.subplot(100, 4, i + 1)\n",
    "    plt.imshow(image.reshape(image_shape), cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below PCA is performed for the given data set. The obtained components (eigenvectors of the covariance matrix of the data) are then displayed as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "faces_centered = faces - faces.mean(axis=0)\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(faces_centered)\n",
    "image_shape = (64, 64)\n",
    "plt.figure(figsize = (3 * 4, 3* 100))\n",
    "for i, image in enumerate(pca.components_[:20,:]):\n",
    "    plt.subplot(100, 4, i + 1)\n",
    "    plt.imshow(image.reshape(image_shape), cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 5 (1,5 pt)**\n",
    "\n",
    "Reduce the number of features describing face images using PCA. Use the function implemented in Excercise 5 to decide on the final number of new features. Perform training and testing of a selected classifier recognizing people in photos.\n",
    "\n",
    "Note 1: the number of features after transformation will be equal to the value of `n_components` or to the number of images in the set if there are less images than` n_components`.\n",
    "\n",
    "Note 2: PCA should be performed on the basis of training data, while the test data should be transformed using the transformation matrix obtained by running PCA on the training data. Explain why it is not appropriate to use PCA on the complete data set (training + testing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 6 (1 pt)**\n",
    "\n",
    "Perform the same experiment as in Excercise 6 but using one of feature selection methods instead of PCA. Compare the results, paying attention to the number of features needed to be selected to get the result on the same level as in Excersice 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Parameter tuning\n",
    "\n",
    "The `GridSearchCV` class enables training of the model and testing it for various parameters and the selection of a set of optimum parameters from the point of view of a given criterion (e.g. in the case of classifiers, this criterion is the minimum classification error). Testing the model for different parameter values is done in a cross-validation process. \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "In the example below, a decision tree for different attribute selection criteria and different values for the minimum number of examples in leaves will be trained for `digits` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix #, accuracy_score\n",
    "data_train, data_test, target_train, target_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=0)\n",
    "tree = DecisionTreeClassifier()\n",
    "parameters ={'criterion': ['gini', 'entropy'], 'min_samples_leaf': [5,4,3]}\n",
    "search = GridSearchCV(tree, parameters, cv=5)\n",
    "search.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter values with the best score are saved in the `best_params_` attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed results for each parameter set can be read from the attribute `cv_results_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model for the best parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.score(data_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline processing\n",
    "\n",
    "If our process consists of several stages, the `Pipeline` class can be used.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "The following example defines a process consisting of a feature extractor (PCA) and a classifier (decision tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('extract', PCA()), ('classify', DecisionTreeClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelining is usually combined with the search for the optimum set of parameters. As a parameter of the function `GridSearchCV`, you should pass an object representing our multi-stage process. In the example below, optimum value of the number of components of PCA is seached in the defined pipeline. The names of the parameters are created by combining the name of the stage (`extract`) with the name of the appropriate parameter (` n_components`) using underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'extract__n_components': [10,20,30]}\n",
    "search = GridSearchCV(pipe, parameters, cv=5)\n",
    "search.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.score(data_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 7 (1,5 pt)**\n",
    "\n",
    "For the selected data set, design a process consisting of feature selection, feature extraction and classifier training. Optimum selection and extraction parameters should be adjusted using the `GridSearchCV` class. You can choose any classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "3477ec1e9823190bffa1d7db9687525a25c42b8a9f242854454b79972d700023"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
